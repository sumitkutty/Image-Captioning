{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from utils import load_caption_data, create_caption_dict, load_train_image_names, load_image, load_npy_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below file contains text that consists of imagename and 5 captions for each image\n",
    "meta_file = 'dataset/Flickr8k_text/Flickr8k.token.txt'\n",
    "\n",
    "#Load the meta file\n",
    "text = load_caption_data(meta_file)\n",
    "\n",
    "# Create a dictionary with {image_name:[caption1, caption2,.....], ....}\n",
    "caption_dict = create_caption_dict(text)\n",
    "\n",
    "# File with image names of train files\n",
    "train_path_file = 'dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "\n",
    "train_img_names = set(load_train_image_names(train_path_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Feature Extractor Model\n",
    "* Create a new model by removing the dense layers and only keep the feature extraction layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport tensorflow as tf\\nfrom tensorflow.keras.applications import InceptionV3 as inception\\nfrom tqdm import tqdm\\n#Define Model\\nmodel = inception(include_top = False, weights = 'imagenet')\\n\\ninput_layer = model.input\\nintermediate_layer = model.layers[-1].output\\n\\n#Group a new model to extract features only (not classify)\\nnew_model = tf.keras.Model(input_layer, intermediate_layer)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3 as inception\n",
    "from tqdm import tqdm\n",
    "#Define Model\n",
    "model = inception(include_top = False, weights = 'imagenet')\n",
    "\n",
    "input_layer = model.input\n",
    "intermediate_layer = model.layers[-1].output\n",
    "\n",
    "#Group a new model to extract features only (not classify)\n",
    "new_model = tf.keras.Model(input_layer, intermediate_layer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_img_paths = sorted([image_files + name + '.jpg' for name in train_img_names])\\n\\nimage_dataset = tf.data.Dataset.from_tensor_slices(train_img_paths)\\n\\nimage_dataset = image_dataset.map(load_image, num_parallel_calls=2).batch(16)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "image_files = 'dataset/Flicker8k_Dataset/'\n",
    "'''\n",
    "train_img_paths = sorted([image_files + name + '.jpg' for name in train_img_names])\n",
    "\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(train_img_paths)\n",
    "\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=2).batch(16)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the features using the model\n",
    "* Saves the encoded image tensors(.npy) files to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor img, path in tqdm(image_dataset):\\n    features = new_model(img)\\n    \\n    \\n    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\\n\\n    #The feature is now of dim 16, 7,7, 2048\\n\\n    for feature, pth in zip(features, path):\\n        #Convert path from tensor to string\\n        path_to_feature = pth.numpy().decode('utf-8')\\n        np.save(path_to_feature, feature.numpy())\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for img, path in tqdm(image_dataset):\n",
    "    features = new_model(img)\n",
    "    \n",
    "    \n",
    "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "\n",
    "    #The feature is now of dim 16, 7,7, 2048\n",
    "\n",
    "    for feature, pth in zip(features, path):\n",
    "        #Convert path from tensor to string\n",
    "        path_to_feature = pth.numpy().decode('utf-8')\n",
    "        np.save(path_to_feature, feature.numpy())\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caption Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caption_handler import clean_captions, add_sof_eof, add_token, create_tokenizer, prepare_training_data\n",
    "\n",
    "\n",
    "#Preprocess the captions (text)\n",
    "img_dict = clean_captions(caption_dict)\n",
    "\n",
    "#Adds indicator for start and end of sequence\n",
    "train_dict = add_token(img_dict, train_img_names)\n",
    "\n",
    "\n",
    "# Create Vocabulary\n",
    "tokenizer, vocab_size, caption_max_len = create_tokenizer(train_dict)\n",
    "\n",
    "#Create X and Y for training\n",
    "xtrain, ytrain = prepare_training_data(train_dict, tokenizer, caption_max_len, vocab_size, image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the stored npy files and create TF dataset\n",
    "* The .npy files are encoded image tensors created by the inception feature extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tf dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((xtrain, ytrain))\n",
    "\n",
    "# Loading the encoded image files (.npy) in parallel and updating the tf dataset by replacing the img path with the encoded tensor\n",
    "dataset = dataset.map(lambda name, cap: tf.numpy_function(load_npy_files, [name, cap], [tf.float32, tf.int32]), num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (<unknown>, <unknown>), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shuffle the dataset\n",
    "buffer_size = 1000\n",
    "batch_size  = 32\n",
    "dataset = dataset.shuffle(buffer_size).batch(32)\n",
    "\n",
    "#PREFETCH THE DATASET \n",
    "dataset = dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (<unknown>, <unknown>), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a83ba41be1a1aa1cdc6ec82c58e6349ca1f74e361e36e16e3867736bbc7e5614"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('CV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
