{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import load_caption_data, create_caption_dict, load_train_image_names, load_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below file contains text that consists of imagename and 5 captions for each image\n",
    "meta_file = 'dataset/Flickr8k_text/Flickr8k.token.txt'\n",
    "\n",
    "#Load the meta file\n",
    "text = load_caption_data(meta_file)\n",
    "\n",
    "# Create a dictionary with {image_name:[caption1, caption2,.....], ....}\n",
    "caption_dict = create_caption_dict(text)\n",
    "\n",
    "# File with image names of train files\n",
    "train_path_file = 'dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "\n",
    "train_img_names = set(load_train_image_names(train_path_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "* Create a new model by removing the dense layers and only keep the feature extraction layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport tensorflow as tf\\nfrom tensorflow.keras.applications import InceptionV3 as inception\\nfrom tqdm import tqdm\\n#Define Model\\nmodel = inception(include_top = False, weights = 'imagenet')\\n\\ninput_layer = model.input\\nintermediate_layer = model.layers[-1].output\\n\\n#Group a new model to extract features only (not classify)\\nnew_model = tf.keras.Model(input_layer, intermediate_layer)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3 as inception\n",
    "from tqdm import tqdm\n",
    "#Define Model\n",
    "model = inception(include_top = False, weights = 'imagenet')\n",
    "\n",
    "input_layer = model.input\n",
    "intermediate_layer = model.layers[-1].output\n",
    "\n",
    "#Group a new model to extract features only (not classify)\n",
    "new_model = tf.keras.Model(input_layer, intermediate_layer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_img_paths = sorted([image_files + name + '.jpg' for name in train_img_names])\\n\\nimage_dataset = tf.data.Dataset.from_tensor_slices(train_img_paths)\\n\\nimage_dataset = image_dataset.map(load_image, num_parallel_calls=2).batch(16)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "image_files = 'dataset/Flicker8k_Dataset/'\n",
    "'''\n",
    "train_img_paths = sorted([image_files + name + '.jpg' for name in train_img_names])\n",
    "\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(train_img_paths)\n",
    "\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=2).batch(16)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor img, path in tqdm(image_dataset):\\n    features = new_model(img)\\n    \\n    \\n    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\\n\\n    #The feature is now of dim 16, 7,7, 2048\\n\\n    for feature, pth in zip(features, path):\\n        #Convert path from tensor to string\\n        path_to_feature = pth.numpy().decode('utf-8')\\n        np.save(path_to_feature, feature.numpy())\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for img, path in tqdm(image_dataset):\n",
    "    features = new_model(img)\n",
    "    \n",
    "    \n",
    "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "\n",
    "    #The feature is now of dim 16, 7,7, 2048\n",
    "\n",
    "    for feature, pth in zip(features, path):\n",
    "        #Convert path from tensor to string\n",
    "        path_to_feature = pth.numpy().decode('utf-8')\n",
    "        np.save(path_to_feature, feature.numpy())\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caption Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caption_handler import clean_captions, add_sof_eof, add_token, create_tokenizer\n",
    "\n",
    "\n",
    "#Preprocess the captions (text)\n",
    "img_dict = clean_captions(caption_dict)\n",
    "\n",
    "#Adds indicator for start and end of sequence\n",
    "train_dict = add_token(img_dict, train_img_names)\n",
    "\n",
    "\n",
    "# Create Vocabulary\n",
    "tokenizer, vocab_size, caption_max_len = create_tokenizer(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_text(text, max_length):\n",
    "    return pad_sequences([text], maxlen = max_length, padding = 'post')[0]\n",
    "\n",
    "def prepare_training_data(data_dict, tokenizer, max_length, vocab_size):\n",
    "    x , y = list() ,list()\n",
    "    for img_name, captions in data_dict.items():\n",
    "        img_path = image_files + img_name + '.jpg'\n",
    "\n",
    "        for caption in captions:\n",
    "            #converts the text sentences to sequences of numbers where the nums are the word's index in vocab\n",
    "            words_ids = tokenizer.texts_to_sequences([caption])[0] \n",
    "            \n",
    "            #Makes all words_ids vector of same length by padding 0's at the end(padding='post') of the vector\n",
    "            padded_ids = pad_text(words_ids,max_length)\n",
    "\n",
    "            x.append(img_path)\n",
    "            y.append(padded_ids)\n",
    "            \n",
    "    return np.array(x), np.array(y)       \n",
    "\n",
    "\n",
    "xtrain, ytrain = prepare_training_data(train_dict, tokenizer, caption_max_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a83ba41be1a1aa1cdc6ec82c58e6349ca1f74e361e36e16e3867736bbc7e5614"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('CV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
